Current cuda device:  0
[Hydra] Optimizer ▶ AdamW
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=amp_enabled)
[Hydra] loss_func ▶ L1Loss()
[Hydra] Scheduler ▶ <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x77254aa7bdd0>
/home/swpants05/Desktop/2025_FastMri/Data/train
/home/swpants05/Desktop/2025_FastMri/Data/val
Epoch # 0 ............... varnet_small ...............
Error executing job with overrides: []                                
Traceback (most recent call last):
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/main.py", line 95, in main
    train(args)   # utils.learning.train_part.train 호출 :contentReference[oaicite:2]{index=2}
    ^^^^^^^^^^^
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py", line 265, in train
    train_loss, train_time = train_epoch(args, epoch, model,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py", line 48, in train_epoch
    for iter, data in pbar:
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 733, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 789, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 211, in collate
    return [
           ^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 212, in <listcomp>
    collate(samples, collate_fn_map=collate_fn_map)
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 155, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 272, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [1, 1, 372, 1] at entry 0 and [1, 1, 368, 1] at entry 1

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
