Current cuda device:  0
[Hydra-visLogging]  True
[Hydra-receptiveField]  False
[Hydra-maskDuplicate] {'enable': True, '_target_': 'utils.data.duplicate_dataset.DuplicateMaskDataset', 'accel_cfgs': [{'accel': 4}, {'accel': 8}], 'bundle_path': '/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek//metaData/precomputed_masks.npz'}
[Hydra] training: accum_steps=10 checkpointing=False amp_enabled=True
[Hydra] grad_clip: enable=False max_norm=1.0 norm_type=2
[Hydra-eval] {'enable': False, 'stages': [{'epoch': 10, 'ssim': 0.9}, {'epoch': 20, 'ssim': 0.95}, {'epoch': 25, 'ssim': 0.96}]}
[Hydra-eval] early_enabled=False, stage_table={10: 0.9, 20: 0.95, 25: 0.96}
[Hydra-model] model_cfg={'_target_': 'utils.model.feature_varnet.FlexibleCascadeVarNet', 'variant': 'dlka', 'cascade_counts': [3, 3, 6], 'feature_chans': 9, 'unet_chans': 32, 'pools': 4, 'sens_chans': 8, 'sens_pools': 4, 'mask_center': True, 'kspace_mult_factor': 1000000.0, 'crop_size': 'none'}
[Hydra] loss_func ‚ñ∂ SSIML1Loss(
  (ssim_base): SSIMLoss()
)
[Hydra] Optimizer ‚ñ∂ NAdam
[Hydra] Scheduler ‚ñ∂ <torch.optim.lr_scheduler.ExponentialLR object at 0x74a7aca4ebd0>
[DeepSpeed] use_deepspeed False
{'enable': False, 'config': {'train_micro_batch_size_per_gpu': 1, 'dist_init_required': False, 'optimizer': {'type': 'Adam', 'params': {'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}}, 'zero_optimization': {'stage': 1, 'offload_optimizer': {'device': 'cpu'}}, 'fp16': {'enabled': False}, 'scheduler': {'type': 'WarmupCosineLR', 'params': {'warmup_min_ratio': 0.001, 'cos_min_ratio': 0.1, 'warmup_num_steps': 2964, 'total_num_steps': 29640}}}}
[Hydra] AugmenterÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
[MRAug] max_epochs :  30
{'_target_': 'utils.augmentations.mraugmenter.MRAugmenter', 'aug_on': True, 'aug_strength': 0.7, 'aug_delay': 5, 'weight_dict': {'fliph': 0.5, 'flipv': 0.5, 'rotate': 0.25, 'scale': 0.5, 'shift': 0.2, 'shear': 0.5}, 'aug_schedule_mode': 'epoch', 'aug_schedule_type': 'ramp', 'max_epochs': 30, 'val_loss_window_size': 5, 'val_loss_grad_start': -0.05, 'val_loss_grad_plateau': -0.001, 'aug_exp_decay': 6.0, 'max_rotation_angle': 15.0, 'scale_range': [0.85, 1.15], 'shift_extent': 5.0, 'max_shear_angle': 10.0}
[Hydra] mask_augmenter :  True
[Mask Aug] max_epochs :  30
{'enable': True, '_target_': 'utils.augmentations.maskaugmenter.MaskAugmenter', 'aug_on': True, 'aug_strength': 0.7, 'aug_delay': 5, 'aug_schedule_mode': 'epoch', 'aug_schedule_type': 'ramp', 'max_epochs': 30, 'val_loss_window_size': 5, 'val_loss_grad_start': -0.05, 'val_loss_grad_plateau': -0.001, 'aug_exp_decay': 6.0, 'mask_specs': {'equispaced': {'prob': 0.4, 'accel': [4, 8], 'cf': [0.07, 0.1]}, 'equispaced_fraction': {'prob': 0.3, 'accel': [4, 8], 'cf': [0.07, 0.1]}, 'random': {'prob': 0.2, 'accel': [4, 8], 'cf': [0.07, 0.1]}, 'magic_fraction': {'prob': 0.1, 'accel': [4, 8], 'cf': [0.07, 0.1]}}, 'allow_any_combination': True}
[Resume] None
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:514: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=amp_enabled)
/home/swpants05/Desktop/2025_FastMri/Data/train
/home/swpants05/Desktop/2025_FastMri/Data/val
[Hydra-eval] {'enable': True, 'every_n_epochs': 1, 'batch_size': 1, 'leaderboard_root': '/home/swpants05/Desktop/2025_FastMri/Data/leaderboard/', 'output_key': 'reconstruction'}
[Hydra-eval] lb_enable=True, lb_every=1
Epoch # 0 ............... fivarnet(6,9,8)-clip_accumSched ...............
Epoch[ 0/30]/:   0%|                         | 0/9874 [00:00<?, ?it/s]/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=amp_enabled):
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/model/feature_varnet/blocks.py:197: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /pytorch/aten/src/ATen/native/TensorCompare.cpp:611.)
  torch.where(
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")

  [VRAM at iter 5000] Allocated: 8258.93 MB | Peak: 10281.18 MB
loss file saved! /home/swpants05/Desktop/2025_FastMri/result/fivarnet(6,9,8)-clip_accumSched/val_loss_log
visual Logging...
[LeaderBoard] Epoch 1: reconstruct & eval ÏãúÏûë
[acc4] save ‚Üí /home/swpants05/Desktop/2025_FastMri/result/fivarnet(6,9,8)-clip_accumSched/reconstructions_leaderboard/acc4
checkpoint_epoch :  1 best_val_loss :  0.06963368505239487
Epoch[ 1/30]/:   0%|                         | 0/9874 [00:00<?, ?it/s]/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[acc8] save ‚Üí /home/swpants05/Desktop/2025_FastMri/result/fivarnet(6,9,8)-clip_accumSched/reconstructions_leaderboard/acc8
checkpoint_epoch :  1 best_val_loss :  0.06963368505239487
[LeaderBoard] acc4=0.9482  acc8=0.9267  mean=0.9375  (8.1 min)
Epoch = [   0/  30] TrainLoss = 0.07623 ValLoss = 0.06963 ValSSIM = 0.9385 TrainTime = 9768.4774s ValTime = 230.7576s
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@NewRecord@@@@@@@@@@@@@@@@@@@@@@@@@@@@
ForwardTime = 1.3411s
Epoch # 1 ............... fivarnet(6,9,8)-clip_accumSched ...............
  with autocast(enabled=amp_enabled):
Epoch[ 1/30]/:  25%|‚ñè| 2455/9874 [40:49<2:09:37,  1.05s/it, loss=0.025
