Current cuda device:  0
[Hydra] Optimizer ▶ Adafactor
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:210: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=amp_enabled)
[Hydra] loss_func ▶ SSIMLoss()
[Hydra] Scheduler ▶ <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7030dc1c9cd0>
/home/swpants05/Desktop/2025_FastMri/Data/train
/home/swpants05/Desktop/2025_FastMri/Data/val
Epoch # 0 ............... varnet_small ...............
Val  [ 0/1]:   0%|                            | 0/863 [00:00<?, ?it/s]/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/model/varnet.py:302: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /pytorch/aten/src/ATen/native/TensorCompare.cpp:611.)
  soft_dc = torch.where(mask, current_kspace - ref_kspace, zero) * self.dc_weight
/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
loss file saved! /home/swpants05/Desktop/2025_FastMri/result/small_varnet_resume_test/val_loss_log
  warnings.warn(
Error executing job with overrides: ['optimizer=adafactor']
Traceback (most recent call last):
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/main.py", line 95, in main
    train(args)   # utils.learning.train_part.train 호출 :contentReference[oaicite:2]{index=2}
    ^^^^^^^^^^^
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py", line 265, in train
    scheduler.step()
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 196, in step
    values = self.get_lr()
             ^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 1008, in get_lr
    return [
           ^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 1011, in <listcomp>
    * (group["lr"] - self.eta_min)
       ~~~~~~~~~~~~^~~~~~~~~~~~~~
TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
