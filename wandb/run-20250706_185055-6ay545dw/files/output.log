Current cuda device:  0
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=amp_enabled)
[Hydra] loss_func ▶ SSIMLoss()
[Hydra] Scheduler ▶ <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x71e207621c50>
/home/swpants05/Desktop/2025_FastMri/Data/train
/home/swpants05/Desktop/2025_FastMri/Data/val
Epoch # 0 ............... varnet_small ...............
Epoch[ 0/5]/:   0%|                          | 0/4937 [00:00<?, ?it/s]/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
False 1
  with autocast(enabled=amp_enabled):
/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/model/varnet.py:286: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /pytorch/aten/src/ATen/native/TensorCompare.cpp:611.)
  soft_dc = torch.where(mask, current_kspace - ref_kspace, zero) * self.dc_weight
max alloc MB: 9582.68701171875
Error executing job with overrides: []                                
Traceback (most recent call last):
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/main.py", line 135, in main
    train(args)   # utils.learning.train_part.train 호출 :contentReference[oaicite:2]{index=2}
    ^^^^^^^^^^^
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py", line 253, in train
    train_loss, train_time = train_epoch(args, epoch, model,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/Desktop/2025_FastMri/2025_GeekSeek/utils/learning/train_part.py", line 76, in train_epoch
    loss.backward()
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/swpants05/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 1118, in unpack_hook
    args = ctx.get_args(ctx.saved_tensors)
                        ^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 9, 768, 400]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
