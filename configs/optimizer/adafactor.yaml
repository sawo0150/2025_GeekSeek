# configs/optimizer/adafactor.yaml
# ── PyTorch 내장 Adafactor (v2.7.1) ─────────────────────────────────
_target_: torch.optim.Adafactor

# explicit 학습률 지정 (None 대신 숫자로)
lr: 1e-3                          # 내부 스케줄 대신 고정 LR 사용

# β2 지수 감쇠율 (논문 권장값)
beta2_decay: -0.8                

# 안정화용 ε1·ε2: 반드시 숫자로
eps: [1e-30, 1e-3]                

# 그라디언트 클리핑 문턱값
d: 1.0                           

# L2 가중치 감쇠
weight_decay: 0.0                

# foreach/fused 구현 토글 (필요 없으면 False)
foreach: false                   
maximize: false                  
