# configs/optim/adagrad.yaml
_target_: torch.optim.Adagrad
lr: 1e-2
lr_decay: 0.0          # 학습률 감쇠
weight_decay: 0.0
eps: 1e-10